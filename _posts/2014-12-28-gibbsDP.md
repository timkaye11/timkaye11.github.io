---
layout: post
title: Gibbs Sampling with the Dirichlet Process
---

### Background

a few days ago, i talked about a clustering algorithm that utilizes the dirichlet process, a widely used tool in the world of bayesian non-parametrics. while the infinite mixture model approach has some cool features, there is definitely some room for improvement in that it is difficult to choose the correct value of alpha - the concentration parameter. therefore we turn to [gibbs sampling](http://en.wikipedia.org/wiki/Gibbs_sampling) which is a MCMC (markov-chain monte carlo) sampling algorithm. 

### Hyper-Priors

suppose that we two-dimensional data and we have some vague prior knowledge on how our data points (observations) are distributed. using hyper-priors (priors on priors), we can use a gibbs sampler to optimally distinguish the clusters in our data. more information about this topic is available [here](http://www.cs.princeton.edu/courses/archive/fall11/cos597C/reading/Neal2000a.pdf). 

let's suppose that the number of clusters in the data is normally distributed (this is our prior belief). we can then place priors on the μ (mean) and the σ<sup>2</sup> (variance), known as hyper-priors. we also need to place a prior on alpha, the concentration parameter of the dirichlet process. 

**more to come...**